Groq x McLaren from Mariah Larwood on Vimeo

![video thumbnail](https://i.vimeocdn.com/video/2096592502-c186643621c7ab96095051eb5d33032d64dad98a53a4742acf1ce0589457cde0-d?mw=80&q=85)

Playing in picture-in-picture

Unmute

Pause

Settings

SpeedNormal

Inference is Fuel for AI

# Groq delivers fast, low cost inference that doesn’t flake when things get real.

[Get Started](https://console.groq.com/)

[![](https://cdn.sanity.io/images/chol0sk5/production/8776faec2ef547091786cde2fca3aaa3ca1a2fc6-423x89.svg)\\
\\
Speed at a winning cost\\
**The McLaren F1 Team chooses Groq for inference globally.**](https://groq.com/newsroom/mclaren-racing-announces-groq-as-an-official-partner-of-the-mclaren-formula-1-team)[![](https://groq.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fchol0sk5%2Fproduction%2F693a17047d16862b17f065843012ca94f9342354-570x321.png&w=1200&q=75)\\
\\
The Groq LPU\\
**LPU built for inference, exceptional speed and affordability at scale.**](https://groq.com/lpu-architecture)

Close

Try the speed of Groq...

Born for this. Literally.

## To deliver different results, you need a different stack.

## Others rely on GPUs alone. Our edge? Custom silicon.

Groq pioneered the LPU in 2016, the first chip purpose-built for inference. Every design choice focuses on keeping intelligence fast and affordable.

[Learn More](https://groq.com/lpu-architecture)

![](https://cdn.sanity.io/images/chol0sk5/production/3d796e873b60ae39c9312c1047fa2bddb4d28d55-1772x1006.png?w=1400&q=80&auto=format)

Benchmarks don’t ship. Workloads do.

## Instant intelligence. Deployed worldwide.

Inference works best when it’s local. Groq’s LPU-based stack runs in data centers across the world to deliver low-latency responses from the most intelligent models.

[View Models](https://groq.com/pricing)

The LPU is the cartridge. GroqCloud is the console.

## Devs trust GroqCloud for inference that stays smart, fast and affordable.

[View Pricing](https://groq.com/pricing)

### What inference provider are you using or considering using to access models?

![](https://cdn.sanity.io/images/chol0sk5/production/8e405936c0105307513ab43b5ec114511aa90a17-1390x515.svg?)![](https://cdn.sanity.io/images/chol0sk5/production/25aa0f27b2ab8a842cca0ecbcf8a17fc3c798743-634x515.svg?)

Source: Artificial Analysis AI Adoption Survey 2025

![](https://groq.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fchol0sk5%2Fproduction%2Fb4943a3c858ba0a814e1af63642bfc7f4967ed0b-4032x2034.png%3Ffm%3Dwebp&w=3840&q=75)

Partnership Spotlight

## The McLaren Formula 1 Team chooses Groq for inference.

The McLaren F1 Team is fueled by decision-making, analysis, development and real-time insights. So the McLaren F1 Team chose Groq.

[Read More](https://groq.com/newsroom/mclaren-racing-announces-groq-as-an-official-partner-of-the-mclaren-formula-1-team)

![](https://cdn.sanity.io/images/chol0sk5/production/ca2f83e820424349a07cc3cd1c7daa8d10fc9977-351x89.svg)

Don’t take our word for it.

## Proof from the people shipping.

[Read Customer Stories](https://groq.com/case-studies)

> If we have things where performance matters more, we come to Groq - you deliver real, working solutions, not just buzzwords.

> We optimized our infrastructure to its limits – but the breakthrough came with GroqCloud. Overnight, our chat speed surged 7.41x while costs fell by 89%. I was stunned. So, we tripled our token consumption. We simply can’t get enough.

> Groq has created immense savings and reduced so much overhead for us. We’ve been able to keep costs for our main offerings incredibly low, helping keep our premium plan at a reasonable price for students of all backgrounds.

> If we have things where performance matters more, we come to Groq - you deliver real, working solutions, not just buzzwords.

> We optimized our infrastructure to its limits – but the breakthrough came with GroqCloud. Overnight, our chat speed surged 7.41x while costs fell by 89%. I was stunned. So, we tripled our token consumption. We simply can’t get enough.

> Groq has created immense savings and reduced so much overhead for us. We’ve been able to keep costs for our main offerings incredibly low, helping keep our premium plan at a reasonable price for students of all backgrounds.

SWITCH FASTER THAN YOU CAN READ THIS.

## OpenAI compatible in just two lines.

[Start Now](https://console.groq.com/)

```python
1import os
2import openai
3
4client = openai.OpenAI(
5  base_url="https://api.groq.com/openai/v1",
6  api_key=os.environ.get("GROQ_API_KEY")
7)
```

Copy

* * *

News

## Featured

- [September 17, 2025 **Groq Raises $750 Million as Inference Demand Surges** Arrow icon pointing right](https://groq.com/newsroom/groq-raises-750-million-as-inference-demand-surges)
- [August 5, 2025 **Day Zero Support for OpenAI Open Models** Arrow icon pointing right](https://groq.com/blog/day-zero-support-for-openai-open-models)
- [May 27, 2025 **From Speed to Scale: How Groq Is Optimized for MoE & Other Large Models** Arrow icon pointing right](https://groq.com/blog/from-speed-to-scale-how-groq-is-optimized-for-moe-other-large-models)

## Build Fast

Seamlessly integrate Groq starting with just a few lines of code

[Try Groq for Free](https://console.groq.com/?utm_source=website&utm_medium=outbound_link&utm_campaign=dev_console_click)